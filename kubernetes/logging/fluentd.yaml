apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: kube-logging
  labels:
    app: fluentd

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
  labels:
    app: fluentd
rules:
- apiGroups: [""]
  resources: ["pods", "namespaces"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: kube-logging

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-logging
  labels:
    app: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
      #Â Expose fluentd metrics
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '24231'
        prometheus.io/path: '/metrics'
    spec:
      serviceAccount: fluentd
      serviceAccountName: fluentd
      # Next, we define a NoSchedule toleration to match the equivalent taint on Kubernetes master nodes. 
      # This will ensure that the DaemonSet also gets rolled out to the Kubernetes masters. 
      # If you don't want to run a Fluentd Pod on your master nodes, remove this toleration. 
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1.4.2-debian-elasticsearch-1.1
        env:
          - name:  FLUENT_ELASTICSEARCH_HOST
            value: "elasticsearch.kube-logging.svc.cluster.local"
          - name:  FLUENT_ELASTICSEARCH_PORT
            value: "9200"
          - name: FLUENT_ELASTICSEARCH_SCHEME
            value: "http"
          - name: FLUENTD_SYSTEMD_CONF
            value: disable
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      # - name: fluentd-config
      #   configMap:
      #     name: fluentd-config

# ---
# apiVersion: v1
# kind: ConfigMap
# metadata:
#   name: fluentd-config
#   namespace: kube-logging
# data:
#   fluent.conf: |
#     <match fluentd.**>
#         @type null
#     </match>

#     # here we read the logs from Docker's containers and parse them
#     <source>
#       @type tail
#       path /var/log/containers/*.log
#       pos_file /var/log/fluentd-containers.log.pos
#       tag kubernetes.*
#       read_from_head true
#       format json
#     </source>

#     # we use kubernetes metadata plugin to add metadatas to the log
#     <filter kubernetes.**>
#         @type kubernetes_metadata
#     </filter>

#     # remove fluentd logs
#     <match kubernetes.var.log.containers.fluentd**.log>
#       @type null
#     </match>

#     <match kubernetes.var.log.containers.**_kube-system_**>
#       @type null
#     </match>

#     # collect exceptions
#     # <match **>
#     #   @type detect_exceptions
#     #   message log
#     #   languages java
#     #   multiline_flush_interval 0.1
#     # </match>

#     # send the logs to Elasticsearch
#     <match kubernetes.**>
#         @type elasticsearch
#         include_tag_key true
#         host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
#         port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
#         scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
#         ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
#         reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'true'}"
#         logstash_prefix "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'logstash'}"
#         logstash_format true
#         <buffer>
#           flush_thread_count "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_THREAD_COUNT'] || '8'}"
#           flush_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL'] || '5s'}"
#           chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '2M'}"
#           queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '32'}"
#           retry_max_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_RETRY_MAX_INTERVAL'] || '30'}"
#           retry_forever true
#         </buffer>
#     </match>
